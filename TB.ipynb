{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 24.6 GiB for an array with shape (4200, 512, 512, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=57'>58</a>\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mtb_detection_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=60'>61</a>\u001b[0m     main()\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=29'>30</a>\u001b[0m all_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(normal_labels \u001b[39m+\u001b[39m tb_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=31'>32</a>\u001b[0m \u001b[39m# Normalize images\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=32'>33</a>\u001b[0m all_images \u001b[39m=\u001b[39m all_images \u001b[39m/\u001b[39;49m \u001b[39m255.0\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=34'>35</a>\u001b[0m \u001b[39m# Split the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W0sdW50aXRsZWQ%3D?line=35'>36</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(all_images, all_labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 24.6 GiB for an array with shape (4200, 512, 512, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "def main():\n",
    "    data_dir = 'C:\\\\Users\\\\WERKIT ZAMBIA\\\\Downloads\\\\TB_Chest_Radiography_Database'\n",
    "\n",
    "    # Load images\n",
    "    normal_images, normal_labels = load_images_from_folder(os.path.join(data_dir, 'Normal'), 0)\n",
    "    tb_images, tb_labels = load_images_from_folder(os.path.join(data_dir, 'Tuberculosis'), 1)\n",
    "\n",
    "    # Combine and convert to numpy arrays\n",
    "    all_images = np.array(normal_images + tb_images)\n",
    "    all_labels = np.array(normal_labels + tb_labels)\n",
    "\n",
    "    # Normalize images\n",
    "    all_images = all_images / 255.0\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the CNN model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(512, 512, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        # Additional layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, validation_split=0.1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save('tb_detection_model.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[516128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=53'>54</a>\u001b[0m     model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mtb_detection_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=55'>56</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=56'>57</a>\u001b[0m     main()\n",
      "\u001b[1;32mUntitled-1.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=23'>24</a>\u001b[0m validation_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39mflow_from_directory(\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m     data_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m     target_size\u001b[39m=\u001b[39m(\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=28'>29</a>\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=29'>30</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=31'>32</a>\u001b[0m \u001b[39m# Define the CNN model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=32'>33</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=33'>34</a>\u001b[0m     Conv2D(\u001b[39m32\u001b[39;49m, (\u001b[39m3\u001b[39;49m, \u001b[39m3\u001b[39;49m), activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m(\u001b[39m256\u001b[39;49m, \u001b[39m256\u001b[39;49m, \u001b[39m3\u001b[39;49m)),\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=34'>35</a>\u001b[0m     MaxPooling2D(\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=35'>36</a>\u001b[0m     \u001b[39m# Additional layers\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=36'>37</a>\u001b[0m     Flatten(),\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=37'>38</a>\u001b[0m     Dense(\u001b[39m128\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=38'>39</a>\u001b[0m     Dense(\u001b[39m1\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=39'>40</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=41'>42</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#W1sdW50aXRsZWQ%3D?line=43'>44</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[0;32m   2101\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   2102\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[0;32m   2103\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[0;32m   2104\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2105\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   2106\u001b[0m     )\n\u001b[0;32m   2107\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[516128,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def main():\n",
    "    data_dir = 'C:\\\\Users\\\\WERKIT ZAMBIA\\\\Downloads\\\\TB_Chest_Radiography_Database'\n",
    "\n",
    "    # Create ImageDataGenerators for training and validation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2  # using 20% of the data for validation\n",
    "    )\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(256, 256),  # reducing image size\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    # Define the CNN model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        # Additional layers\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model.save('tb_detection_model.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3360 images belonging to 2 classes.\n",
      "Found 840 images belonging to 2 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1028, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1122, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\legacy\\serialization.py\", line 557, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: 'recall'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtb_detection_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model with reduced steps per epoch to further save memory\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Half the steps\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Half the steps\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtb_detection_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\WERKIT~1\\AppData\\Local\\Temp\\__autograph_generated_filefj4jybh_.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1028, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1122, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"c:\\Users\\WERKIT ZAMBIA\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\saving\\legacy\\serialization.py\", line 557, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: 'recall'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "def main():\n",
    "    data_dir = 'C:\\\\Users\\\\WERKIT ZAMBIA\\\\Downloads\\\\TB_Chest_Radiography_Database'\n",
    "\n",
    "    # Create ImageDataGenerators for training and validation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2  # using 20% of the data for validation\n",
    "    )\n",
    "\n",
    "    # Reduce batch size to decrease memory usage\n",
    "    batch_size = 16\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "\n",
    "    # Define a simpler CNN model\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['Recall'])\n",
    "\n",
    "    # Train the model with reduced steps per epoch to further save memory\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_generator.samples // batch_size // 2,  # Half the steps\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_generator.samples // batch_size // 2,  # Half the steps\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    model.save('tb_detection_model.h5')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def main():\n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model('tb_detection_model.h5')\n",
    "\n",
    "    # Path to the new image for analysis\n",
    "    image_path = input(\"Enter the path of the image to analyze: \")\n",
    "\n",
    "    # Preprocess the image\n",
    "    img = load_and_preprocess_image(image_path)\n",
    "    img = np.expand_dims(img, axis=0)  # Expand dims to fit model input\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img)\n",
    "    result = 'TB Detected' if prediction[0][0] > 0.5 else 'No TB'\n",
    "    print(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: Werkzeug>=2.3.7 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (2.3.7)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (8.1.3)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (1.6.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flask) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=3.6.0->flask) (3.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\werkit zambia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WERKIT ZAMBIA\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('tb_detection_model.h5')\n",
    "\n",
    "def load_and_preprocess_image(image):\n",
    "    img = cv2.imdecode(np.fromstring(image.read(), np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    img = cv2.resize(img, (256, 256))  # Ensure this matches the training input\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    if request.method == 'POST':\n",
    "        file = request.files['file']\n",
    "        if file:\n",
    "            img = load_and_preprocess_image(file)\n",
    "            img = np.expand_dims(img, axis=0)  # Expand dims to fit model input\n",
    "            prediction = model.predict(img)\n",
    "            result = 'TB Detected' if prediction[0][0] > 0.5 else 'No TB'\n",
    "            return result\n",
    "    return '''\n",
    "    <!doctype html>\n",
    "    <title>Upload an image</title>\n",
    "    <h1>Upload a Chest X-Ray Image for TB Analysis</h1>\n",
    "    <form method=post enctype=multipart/form-data>\n",
    "      <input type=file name=file>\n",
    "      <input type=submit value=Analyze>\n",
    "    </form>\n",
    "    '''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('tb_detection_model.h5')\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    'user': 'root',\n",
    "    'password': '',\n",
    "    'host': 'localhost',\n",
    "    'database': 'tb',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "def load_and_preprocess_image(image):\n",
    "    img = cv2.imdecode(np.fromstring(image.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "def save_result(name, id_number, nrc, date_of_testing, result):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"INSERT INTO users (name, id_number, nrc, date_of_testing, results) VALUES (%s, %s, %s, %s, %s)\"\n",
    "        cursor.execute(query, (name, id_number, nrc, date_of_testing, result))\n",
    "        conn.commit()\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    finally:\n",
    "        if conn is not None and conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "def get_user_details(id_number):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**db_config)\n",
    "        cursor = conn.cursor()\n",
    "        query = \"SELECT * FROM users WHERE id_number = %s\"\n",
    "        cursor.execute(query, (id_number,))\n",
    "        result = cursor.fetchone()\n",
    "        return result\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if conn is not None and conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def upload_file():\n",
    "    user_details = None\n",
    "    search_id = \"\"\n",
    "    user_not_found = False\n",
    "    analysis_result = None  # Variable to store the analysis result\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        if 'file' in request.files:\n",
    "            file = request.files['file']\n",
    "            if file:\n",
    "                img = load_and_preprocess_image(file)\n",
    "                img = np.expand_dims(img, axis=0)  # Expand dims to fit model input\n",
    "                prediction = model.predict(img)\n",
    "                analysis_result = 'TB Detected' if prediction[0][0] > 0.5 else 'No TB'\n",
    "                \n",
    "                \n",
    "                 # Display result and user details form\n",
    "                return '''\n",
    "                    <h1>Result: {}</h1>\n",
    "                    <form method=\"post\">\n",
    "                        <label for=\"name\">Name:</label>\n",
    "                        <input type=\"text\" name=\"name\" required>\n",
    "                        <label for=\"id_number\">ID Number:</label>\n",
    "                        <input type=\"text\" name=\"id_number\" required>\n",
    "                        <label for=\"nrc\">NRC:</label>\n",
    "                        <input type=\"text\" name=\"nrc\" required>\n",
    "                        <label for=\"date_of_testing\">Date of Testing:</label>\n",
    "                        <input type=\"date\" name=\"date_of_testing\" required>\n",
    "                        <input type=\"hidden\" name=\"result\" value=\"{}\">\n",
    "                        <input type=\"submit\" value=\"Upload Results\">\n",
    "                    </form>\n",
    "                '''.format(result, result)\n",
    "        else:\n",
    "            # Save user details and result to database\n",
    "            name = request.form['name']\n",
    "            id_number = request.form['id_number']\n",
    "            nrc = request.form['nrc']\n",
    "            date_of_testing = request.form['date_of_testing']\n",
    "            result = request.form['result']\n",
    "            save_result(name, id_number, nrc, date_of_testing, result)\n",
    "            return '<h1>Results Uploaded Successfully</h1>'\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # Additional logic for saving result if needed\n",
    "        elif 'search_id' in request.form:\n",
    "            search_id = request.form.get('search_id')\n",
    "            user_details = get_user_details(search_id)\n",
    "            if not user_details:\n",
    "                user_not_found = True  # User not found\n",
    "\n",
    "        elif 'name' in request.form:\n",
    "            # Save user details and result to database\n",
    "            name = request.form['name']\n",
    "            id_number = request.form['id_number']\n",
    "            nrc = request.form['nrc']\n",
    "            date_of_testing = request.form['date_of_testing']\n",
    "            result = request.form['result']\n",
    "            save_result(name, id_number, nrc, date_of_testing, result)\n",
    "            return '<h1>Results Uploaded Successfully</h1>'\n",
    "\n",
    "    return render_template('template.html', \n",
    "                           user_details=user_details,\n",
    "                           search_id=search_id,\n",
    "                           user_not_found=user_not_found,\n",
    "                           analysis_result=analysis_result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
